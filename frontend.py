"""
frontend.py

Amazon Bedrock Agentã®ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã€‚

- Streamlitã‚’ç”¨ã„ãŸãƒãƒ£ãƒƒãƒˆUIã®è¡¨ç¤º
- Bedrockã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ã®å¯¾è©±ãƒ»ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
- ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚„ãƒˆãƒ¬ãƒ¼ã‚¹ã‚¤ãƒ™ãƒ³ãƒˆã®å‹•çš„ãªè¡¨ç¤º
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãŠã‚ˆã³ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯

MIT License (c) 2025 y16ra
"""

import os
import json
import uuid
import boto3
import streamlit as st

from dotenv import load_dotenv
from botocore.exceptions import ClientError
from botocore.eventstream import EventStreamError

TITLE = "Chat with Amazon Bedrock Agent"
WELCOME_MESSAGE = "ç”»é¢ä¸‹éƒ¨ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒã‚¯ã‚¹ã‹ã‚‰ä½•ã§ã‚‚è³ªå•ã—ã¦ã­ï¼"

def initialize_session():
    """ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®åˆæœŸè¨­å®šã‚’è¡Œã†"""
    if "client" not in st.session_state:
        st.session_state.client = boto3.client("bedrock-agent-runtime", region_name="us-east-1")
    
    if "session_id" not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())
    
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    if "last_prompt" not in st.session_state:
        st.session_state.last_prompt = None
    
    return st.session_state.client, st.session_state.session_id, st.session_state.messages

def display_chat_history(messages):
    """ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è¡¨ç¤ºã™ã‚‹"""
    st.title(TITLE)
    st.text(WELCOME_MESSAGE)
    for message in messages:
        with st.chat_message(message['role']):
            st.markdown(message['text'])

def handle_trace_event(event):
    """ãƒˆãƒ¬ãƒ¼ã‚¹ã‚¤ãƒ™ãƒ³ãƒˆã®å‡¦ç†ã‚’è¡Œã†"""
    if "orchestrationTrace" not in event["trace"]["trace"]:
        return
    
    trace = event["trace"]["trace"]["orchestrationTrace"]
    
    # ã€Œãƒ¢ãƒ‡ãƒ«å…¥åŠ›ã€ãƒˆãƒ¬ãƒ¼ã‚¹ã®è¡¨ç¤º
    if "modelInvocationInput" in trace:
        with st.expander("ğŸ¤” æ€è€ƒä¸­â€¦", expanded=False):
            input_trace = trace["modelInvocationInput"]["text"]
            try:
                st.json(json.loads(input_trace))
            except:
                st.write(input_trace)
    
    # ã€Œãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã€ãƒˆãƒ¬ãƒ¼ã‚¹ã®è¡¨ç¤º
    if "modelInvocationOutput" in trace:
        output_trace = trace["modelInvocationOutput"]["rawResponse"]["content"]
        with st.expander("ğŸ’¡ æ€è€ƒãŒã¾ã¨ã¾ã‚Šã¾ã—ãŸ", expanded=False):
            try:
                thinking = json.loads(output_trace)["content"][0]["text"]
                if thinking:
                    st.write(thinking)
                else:
                    st.write(json.loads(output_trace)["content"][0])
            except:
                st.write(output_trace)
    
    # ã€Œæ ¹æ‹ ã€ãƒˆãƒ¬ãƒ¼ã‚¹ã®è¡¨ç¤º
    if "rationale" in trace:
        with st.expander("âœ… æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®šã—ã¾ã—ãŸ", expanded=True):
            st.write(trace["rationale"]["text"])
    
    # ã€Œãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã€ãƒˆãƒ¬ãƒ¼ã‚¹ã®è¡¨ç¤º
    if "invocationInput" in trace:
        invocation_type = trace["invocationInput"]["invocationType"]
        
        if invocation_type == "AGENT_COLLABORATOR":
            agent_name = trace["invocationInput"]["agentCollaboratorInvocationInput"]["agentCollaboratorName"]
            with st.expander(f"ğŸ¤– ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€Œ{agent_name}ã€ã‚’å‘¼ã³å‡ºã—ä¸­â€¦", expanded=True):
                st.write(trace["invocationInput"]["agentCollaboratorInvocationInput"]["input"]["text"])
        
        elif invocation_type == "KNOWLEDGE_BASE":
            with st.expander("ğŸ“– ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ä¸­â€¦", expanded=False):
                st.write(trace["invocationInput"]["knowledgeBaseLookupInput"]["text"])
        
        elif invocation_type == "ACTION_GROUP":
            with st.expander("ğŸ’» Lambdaã‚’å®Ÿè¡Œä¸­â€¦", expanded=False):
                st.write(trace['invocationInput']['actionGroupInvocationInput'])
    
    # ã€Œè¦³å¯Ÿã€ãƒˆãƒ¬ãƒ¼ã‚¹ã®è¡¨ç¤º
    if "observation" in trace:
        obs_type = trace["observation"]["type"]
        
        if obs_type == "KNOWLEDGE_BASE":
            with st.expander("ğŸ” ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ¤œç´¢çµæœã‚’å–å¾—ã—ã¾ã—ãŸ", expanded=False):
                st.write(trace["observation"]["knowledgeBaseLookupOutput"]["retrievedReferences"])
        
        elif obs_type == "AGENT_COLLABORATOR":
            agent_name = trace["observation"]["agentCollaboratorInvocationOutput"]["agentCollaboratorName"]
            with st.expander(f"ğŸ¤– ã‚µãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€Œ{agent_name}ã€ã‹ã‚‰å›ç­”ã‚’å–å¾—ã—ã¾ã—ãŸ", expanded=True):
                st.write(trace["observation"]["agentCollaboratorInvocationOutput"]["output"]["text"])

def invoke_bedrock_agent(client, session_id, prompt):
    """Bedrockã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã™"""
    load_dotenv()
    return client.invoke_agent(
        agentId=os.getenv("AGENT_ID"),
        agentAliasId=os.getenv("AGENT_ALIAS_ID"),
        sessionId=session_id,
        enableTrace=True,
        inputText=prompt,
    )

def handle_agent_response(response, messages):
    """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†ã™ã‚‹"""
    with st.chat_message("assistant"):
        for event in response.get("completion"):
            if "trace" in event:
                handle_trace_event(event)
            
            if "chunk" in event:
                answer = event["chunk"]["bytes"].decode()
                st.write(answer)
                messages.append({"role": "assistant", "text": answer})

def show_error_popup(exception):
    """ã‚¨ãƒ©ãƒ¼ãƒãƒƒãƒ—ã‚¢ãƒƒãƒ—ã‚’è¡¨ç¤ºã™ã‚‹"""
    if exception == "dependencyFailedException":
        error_message = "ã€ã‚¨ãƒ©ãƒ¼ã€‘ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®Aurora DBãŒã‚¹ãƒªãƒ¼ãƒ—ã—ã¦ã„ãŸã‚ˆã†ã§ã™ã€‚æ•°ç§’ãŠã„ã¦ã‹ã‚‰ã€ãƒ–ãƒ©ã‚¦ã‚¶ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ğŸ™"
    elif exception == "throttlingException":
        error_message = "ã€ã‚¨ãƒ©ãƒ¼ã€‘Bedrockã®ãƒ¢ãƒ‡ãƒ«è² è·ãŒé«˜ã„ã‚ˆã†ã§ã™ã€‚1åˆ†å¾…ã£ã¦ã‹ã‚‰ã€ãƒ–ãƒ©ã‚¦ã‚¶ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã—ã¦å†åº¦ãŠè©¦ã—ãã ã•ã„ğŸ™ï¼ˆæ”¹å–„ã—ãªã„å ´åˆã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¤‰æ›´ã™ã‚‹ã‹[ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ã‚©ãƒ¼ã‚¿ã®å¼•ãä¸Šã’ç”³è«‹](https://aws.amazon.com/jp/blogs/news/generative-ai-amazon-bedrock-handling-quota-problems/)ã‚’å®Ÿæ–½ãã ã•ã„ï¼‰"
    st.error(error_message)

def main():
    """ãƒ¡ã‚¤ãƒ³ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†"""
    client, session_id, messages = initialize_session()
    display_chat_history(messages)
    
    if prompt := st.chat_input("ä¾‹ï¼šç”»åƒå…¥ã‚Šè³‡æ–™ã‚’ä½¿ã£ãŸRAGã‚¢ãƒ—ãƒªã‚’ä½œã‚‹ã«ã¯ã©ã†ã™ã‚Œã°ã„ã„ï¼Ÿ"):
        messages.append({"role": "human", "text": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        try:
            response = invoke_bedrock_agent(client, session_id, prompt)
            handle_agent_response(response, messages)
            
        except (EventStreamError, ClientError) as e:
            if "dependencyFailedException" in str(e):
                show_error_popup("dependencyFailedException")
            elif "throttlingException" in str(e):
                show_error_popup("throttlingException")
            else:
                raise e

if __name__ == "__main__":
    main()
